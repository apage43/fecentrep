{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fer.data as fecdata\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fecdata.pac_to_pac_transactions()\n",
    "dataset, df, labelers = fecdata.prepare(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "from fer.model import Config, FECEncoder, TabDataset, TabularDenoiser\n",
    "import torch\n",
    "\n",
    "cfg = Config(\n",
    "    embedding_init_std=1/512.,\n",
    "    tied_encoder_decoder_emb=True,\n",
    "    entity_emb_normed=False,\n",
    "    cos_sim_decode_entity=True,\n",
    "    transformer_dim = 384,\n",
    "    transformer_heads = 12,\n",
    "    transformer_layers = 8,\n",
    "    entity_dim = 384,\n",
    ")\n",
    "lr = 1e-3\n",
    "n_epochs = 4\n",
    "model = TabularDenoiser(\n",
    "    cfg,\n",
    "    n_entities=max(dataset[\"src\"].max(), dataset[\"dst\"].max()) + 1,\n",
    "    n_etype=dataset[\"etype\"].max() + 1,\n",
    "    n_ttype=dataset[\"ttype\"].max() + 1,\n",
    ")\n",
    "tds = TabDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "model = model.to(device)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitgen = torch.Generator().manual_seed(41)\n",
    "batch_size=3000    \n",
    "train_set, val_set = random_split(tds, [0.9, 0.1], generator=splitgen)\n",
    "tdl = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    #persistent_workers=True,\n",
    ")\n",
    "vdl = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    #persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lrsched\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=lr,\n",
    ")\n",
    "class WarmupConstantSchedule(lrsched.LambdaLR):\n",
    "    \"\"\" Linear warmup and then constant.\n",
    "        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.\n",
    "        Keeps learning rate schedule equal to 1. after warmup_steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        return 1.\n",
    "\n",
    "class WarmupCosineSchedule(lrsched.LambdaLR):\n",
    "    \"\"\" Linear warmup and then cosine decay.\n",
    "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
    "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
    "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "scheduler = WarmupCosineSchedule(optimizer, 1000, t_total=len(tdl) * n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer.multitask import CoVWeightingLoss, UncertaintyWeightedLoss\n",
    "n_losses = 14\n",
    "# lossweighter = CoVWeightingLoss(n_losses)\n",
    "lossweighter = UncertaintyWeightedLoss(n_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtsks = sorted(k for k in dataset.keys() if k.startswith('scaled_dt_'))\n",
    "def decoder_loss(encoded, batch):\n",
    "    srclogits, dstlogits, etlogits, ttlogits, amtd, amtpos, dt_pred = model.decoder(encoded, model.encoder)\n",
    "    srcloss = F.cross_entropy(srclogits, batch['src'].squeeze())\n",
    "    dstloss = F.cross_entropy(dstlogits, batch['dst'].squeeze())\n",
    "    etloss = F.cross_entropy(etlogits, batch['etype'].squeeze())\n",
    "    ttloss = F.cross_entropy(ttlogits, batch['ttype'].squeeze())\n",
    "    amtloss = F.mse_loss(amtd, batch['amt'])\n",
    "    amtposloss = F.binary_cross_entropy_with_logits(amtpos, batch['amt_pos'].to(torch.float))\n",
    "    #print(dt_pred.shape)\n",
    "    dt_targets = torch.cat([batch[k].squeeze(dim=1) for k in dtsks], dim=1)\n",
    "    #print(dt_targets.shape)\n",
    "    dt_loss = F.mse_loss(dt_pred, dt_targets) \n",
    "    return dict(srcloss=srcloss,dstloss=dstloss,etloss=etloss,ttloss=ttloss,amtloss=amtloss,amtposloss=amtposloss,dt_loss=dt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mapage43\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/helly/repo/fer2/wandb/run-20230731_234236-njjlqq8u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apage43/fecentrep2/runs/njjlqq8u' target=\"_blank\">iconic-plasma-98</a></strong> to <a href='https://wandb.ai/apage43/fecentrep2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apage43/fecentrep2' target=\"_blank\">https://wandb.ai/apage43/fecentrep2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apage43/fecentrep2/runs/njjlqq8u' target=\"_blank\">https://wandb.ai/apage43/fecentrep2/runs/njjlqq8u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ebc04a46994947b53410117d3d32b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-31 23:42:43,203] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:44,707] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:45,517] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:46,169] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:46,913] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:47,433] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:47,945] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:48,452] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:48,956] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-31 23:42:49,465] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    }
   ],
   "source": [
    "with wandb.init(project='fecentrep2', save_code=True, config=dict(lr=lr, **asdict(cfg))) as run:\n",
    "    for epoch in range(n_epochs):\n",
    "        with tqdm(tdl) as t:\n",
    "            for i, batch in enumerate(t):\n",
    "                batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                model.zero_grad()\n",
    "                orig, corrupted, recovered = model(batch)\n",
    "                enclosses = decoder_loss(orig, batch)\n",
    "                reclosses = decoder_loss(recovered, batch)\n",
    "                #distloss = F.mse_loss(orig, recovered)\n",
    "                # margin = 0.1\n",
    "                # ocdiff = (orig != corrupted).max(dim=2).values.max(dim=0).values.float()\n",
    "                # rec_corrupt_err = ((recovered-corrupted).pow(2).mean(dim=2).mean(dim=0) * ocdiff).sum() / ocdiff.sum()\n",
    "                # repel_loss = F.relu(margin - rec_corrupt_err)\n",
    "                all_losses = {}\n",
    "                all_losses.update({f'enc/{k}': v for k,v in enclosses.items()})\n",
    "                all_losses.update({f'rec/{k}': v for k,v in reclosses.items()})\n",
    "                #all_losses['dist_loss'] = distloss\n",
    "                # all_losses['repel_loss'] = repel_loss\n",
    "                weighted_loss = lossweighter.forward([lv for _, lv in sorted(all_losses.items())])\n",
    "                total_loss = weighted_loss\n",
    "                all_losses['total_loss'] = total_loss\n",
    "                wandb.log(dict(**all_losses, lr=scheduler.get_last_lr()[0]))\n",
    "                total_loss.backward()\n",
    "                t.set_postfix(dict(loss=str(total_loss)))\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "    torch.save(model.state_dict(), f'{run.name}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import umap.plot as upl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entemb = model.encoder.entity_embeddings.weight.detach().cpu().numpy()\n",
    "entemb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "id2cid = labelers['id_labeler'].encoder.classes_\n",
    "idorder = pd.DataFrame({'CMTE_ID':id2cid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frame(header_file, data_file, dtypes={}):\n",
    "    header = pd.read_csv(header_file)\n",
    "    dt = {c: str for c in header.columns}\n",
    "    dt.update(dtypes)\n",
    "    data = pd.read_csv(data_file, sep=\"|\", names=header.columns, dtype=dt)\n",
    "    return data\n",
    "\n",
    "def read_cm(year, basedir='./data'):\n",
    "    cm = read_frame(\n",
    "        f\"{basedir}/cm_header_file.csv\",\n",
    "        f\"{basedir}/{year}/cm.txt\",\n",
    "        dtypes={\n",
    "            c: \"str\"\n",
    "            for c in (\n",
    "                \"CMTE_DSGN\",\n",
    "                \"CMTE_TP\",\n",
    "                \"CMTE_PTY_AFFILIATION\",\n",
    "                \"CMTE_FILING_FREQ\",\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    return cm\n",
    "\n",
    "cmdf = idorder.join(pd.concat([read_cm(2020), read_cm(2022), read_cm(2024)]).drop_duplicates(subset=['CMTE_ID'], keep='last').set_index('CMTE_ID'), on='CMTE_ID').dropna(subset=['CMTE_NM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedemb = entemb[cmdf.index]\n",
    "namedemb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "uop = UMAP(verbose=True, metric='cosine')\n",
    "e2d = uop.fit_transform(namedemb)\n",
    "eframe = pd.DataFrame(e2d, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=450\n",
    "(hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='CMTE_PTY_AFFILIATION', cmap='Category20') + \n",
    " hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='CMTE_DSGN', cmap='Category20') + \n",
    " hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='CMTE_TP', cmap='Category20') +\n",
    " hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='ORG_TP', cmap='Category20')).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_atlas(do_norm=True):\n",
    "    from nomic import atlas\n",
    "    from sklearn.preprocessing import normalize\n",
    "    \n",
    "    atlas.map_embeddings(\n",
    "        normalize(namedemb) if do_norm else namedemb,\n",
    "        data=cmdf.reset_index(drop=True),\n",
    "        name='fecentrep-2' + ('-norm' if do_norm else ''),\n",
    "        colorable_fields=['CMTE_TP', 'CMTE_DSGN', 'ORG_TP', 'CMTE_PTY_AFFILIATION'],\n",
    "        id_field='CMTE_ID',\n",
    "        #topic_label_field='CMTE_NM',\n",
    "        reset_project_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_atlas(do_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_atlas(do_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
