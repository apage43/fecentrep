{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fer.data as fecdata\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fecdata.pac_to_pac_transactions()\n",
    "dataset, df, labelers = fecdata.prepare(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer.model import Config, FECEncoder, TabDataset, TabularDenoiser\n",
    "import torch\n",
    "\n",
    "cfg = Config(\n",
    "    embedding_init_std=1/512.,\n",
    "    tied_encoder_decoder_emb=True,\n",
    "    entity_emb_normed=False,\n",
    "    cos_sim_decode_entity=False,\n",
    "    transformer_dim = 384,\n",
    "    transformer_heads = 12,\n",
    "    transformer_layers = 8,\n",
    "    entity_dim = 384,\n",
    ")\n",
    "lr = 1e-3\n",
    "n_epochs = 4\n",
    "model = TabularDenoiser(\n",
    "    cfg,\n",
    "    n_entities=max(dataset[\"src\"].max(), dataset[\"dst\"].max()) + 1,\n",
    "    n_etype=dataset[\"etype\"].max() + 1,\n",
    "    n_ttype=dataset[\"ttype\"].max() + 1,\n",
    ")\n",
    "tds = TabDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "model = model.to(device)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitgen = torch.Generator().manual_seed(41)\n",
    "batch_size=2800    \n",
    "train_set, val_set = random_split(tds, [0.9, 0.1], generator=splitgen)\n",
    "tdl = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    #persistent_workers=True,\n",
    ")\n",
    "vdl = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    #persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lrsched\n",
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=lr,\n",
    ")\n",
    "class WarmupConstantSchedule(lrsched.LambdaLR):\n",
    "    \"\"\" Linear warmup and then constant.\n",
    "        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.\n",
    "        Keeps learning rate schedule equal to 1. after warmup_steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        return 1.\n",
    "\n",
    "class WarmupCosineSchedule(lrsched.LambdaLR):\n",
    "    \"\"\" Linear warmup and then cosine decay.\n",
    "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
    "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
    "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "scheduler = WarmupCosineSchedule(optimizer, 1000, t_total=len(tdl) * n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer.multitask import CoVWeightingLoss, UncertaintyWeightedLoss\n",
    "n_losses = 14\n",
    "# lossweighter = CoVWeightingLoss(n_losses)\n",
    "lossweighter = UncertaintyWeightedLoss(n_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtsks = sorted(k for k in dataset.keys() if k.startswith('scaled_dt_'))\n",
    "def decoder_loss(encoded, batch):\n",
    "    srclogits, dstlogits, etlogits, ttlogits, amtd, amtpos, dt_pred = model.decoder(encoded, model.encoder)\n",
    "    srcloss = F.cross_entropy(srclogits, batch['src'].squeeze())\n",
    "    dstloss = F.cross_entropy(dstlogits, batch['dst'].squeeze())\n",
    "    etloss = F.cross_entropy(etlogits, batch['etype'].squeeze())\n",
    "    ttloss = F.cross_entropy(ttlogits, batch['ttype'].squeeze())\n",
    "    amtloss = F.mse_loss(amtd, batch['amt'])\n",
    "    amtposloss = F.binary_cross_entropy_with_logits(amtpos, batch['amt_pos'].to(torch.float))\n",
    "    #print(dt_pred.shape)\n",
    "    dt_targets = torch.cat([batch[k].squeeze(dim=1) for k in dtsks], dim=1)\n",
    "    #print(dt_targets.shape)\n",
    "    dt_loss = F.mse_loss(dt_pred, dt_targets) \n",
    "    return dict(srcloss=srcloss,dstloss=dstloss,etloss=etloss,ttloss=ttloss,amtloss=amtloss,amtposloss=amtposloss,dt_loss=dt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project='fecentrep2', save_code=True, config=dict(lr=lr, **asdict(cfg))) as run:\n",
    "    for epoch in range(n_epochs):\n",
    "        with tqdm(tdl) as t:\n",
    "            for i, batch in enumerate(t):\n",
    "                batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                model.zero_grad()\n",
    "                orig, corrupted, recovered = model(batch)\n",
    "                enclosses = decoder_loss(orig, batch)\n",
    "                reclosses = decoder_loss(recovered, batch)\n",
    "                #distloss = F.mse_loss(orig, recovered)\n",
    "                # margin = 0.1\n",
    "                # ocdiff = (orig != corrupted).max(dim=2).values.max(dim=0).values.float()\n",
    "                # rec_corrupt_err = ((recovered-corrupted).pow(2).mean(dim=2).mean(dim=0) * ocdiff).sum() / ocdiff.sum()\n",
    "                # repel_loss = F.relu(margin - rec_corrupt_err)\n",
    "                all_losses = {}\n",
    "                all_losses.update({f'enc/{k}': v for k,v in enclosses.items()})\n",
    "                all_losses.update({f'rec/{k}': v for k,v in reclosses.items()})\n",
    "                #all_losses['dist_loss'] = distloss\n",
    "                # all_losses['repel_loss'] = repel_loss\n",
    "                weighted_loss = lossweighter.forward([lv for _, lv in sorted(all_losses.items())])\n",
    "                total_loss = weighted_loss\n",
    "                all_losses['total_loss'] = total_loss\n",
    "                wandb.log(dict(**all_losses, lr=scheduler.get_last_lr()[0]))\n",
    "                total_loss.backward()\n",
    "                t.set_postfix(dict(loss=str(total_loss)))\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "    torch.save(model.state_dict(), f'{run.name}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP\n",
    "# import umap.plot as upl\n",
    "# import holoviews as hv\n",
    "# hv.extension('bokeh')\n",
    "# entemb = model.encoder.entity_embeddings.weight.detach().cpu().numpy()\n",
    "# entemb.shape\n",
    "# import pandas as pd\n",
    "# id2cid = labelers['id_labeler'].encoder.classes_\n",
    "# idorder = pd.DataFrame({'CMTE_ID':id2cid})\n",
    "# def read_frame(header_file, data_file, dtypes={}):\n",
    "#     header = pd.read_csv(header_file)\n",
    "#     dt = {c: str for c in header.columns}\n",
    "#     dt.update(dtypes)\n",
    "#     data = pd.read_csv(data_file, sep=\"|\", names=header.columns, dtype=dt)\n",
    "#     return data\n",
    "\n",
    "# def read_cm(year, basedir='./data'):\n",
    "#     cm = read_frame(\n",
    "#         f\"{basedir}/cm_header_file.csv\",\n",
    "#         f\"{basedir}/{year}/cm.txt\",\n",
    "#         dtypes={\n",
    "#             c: \"str\"\n",
    "#             for c in (\n",
    "#                 \"CMTE_DSGN\",\n",
    "#                 \"CMTE_TP\",\n",
    "#                 \"CMTE_PTY_AFFILIATION\",\n",
    "#                 \"CMTE_FILING_FREQ\",\n",
    "#             )\n",
    "#         },\n",
    "#     )\n",
    "#     return cm\n",
    "\n",
    "# cmdf = idorder.join(pd.concat([read_cm(2020), read_cm(2022), read_cm(2024)]).drop_duplicates(subset=['CMTE_ID'], keep='last').set_index('CMTE_ID'), on='CMTE_ID').dropna(subset=['CMTE_NM'])\n",
    "# namedemb = entemb[cmdf.index]\n",
    "# namedemb.shape\n",
    "# import numpy as np\n",
    "# uop = UMAP(verbose=True, metric='cosine')\n",
    "# e2d = uop.fit_transform(namedemb)\n",
    "# eframe = pd.DataFrame(e2d, columns=['x', 'y'])\n",
    "# sz=450\n",
    "# (hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='CMTE_PTY_AFFILIATION', cmap='Category20') + \n",
    "#  hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='CMTE_DSGN', cmap='Category20') + \n",
    "#  hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='CMTE_TP', cmap='Category20') +\n",
    "#  hv.Points(eframe.join(cmdf.reset_index(drop=True))).opts(width=sz, height=sz, color='ORG_TP', cmap='Category20')).cols(2)\n",
    "# def do_atlas(do_norm=True):\n",
    "#     from nomic import atlas\n",
    "#     from sklearn.preprocessing import normalize\n",
    "    \n",
    "#     atlas.map_embeddings(\n",
    "#         normalize(namedemb) if do_norm else namedemb,\n",
    "#         data=cmdf.reset_index(drop=True),\n",
    "#         name='fecentrep-2' + ('-norm' if do_norm else ''),\n",
    "#         colorable_fields=['CMTE_TP', 'CMTE_DSGN', 'ORG_TP', 'CMTE_PTY_AFFILIATION'],\n",
    "#         id_field='CMTE_ID',\n",
    "#         #topic_label_field='CMTE_NM',\n",
    "#         reset_project_if_exists=True,\n",
    "#     )\n",
    "# # do_atlas(do_norm=True)\n",
    "# # do_atlas(do_norm=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
